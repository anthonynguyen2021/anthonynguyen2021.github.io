---
layout: archive
title: "Publications"
permalink: /publications/
author_profile: true
redirect_from:
  - /publications
---


<b> Papers </b> <br> 
<ol>
  <li>Krishnakumar Balasubramanian, Saeed Ghadimi, Anthony Nguyen. <i> Stochastic Multilevel Composition Optimization Algorithms with Level-Independent Convergence Rates </i>, <a href= "https://arxiv.org/pdf/2008.10526.pdf"> arXiv</a>.</li>
  
  <ul> <li> In this paper, we study smooth stochastic multi-level composition optimization problems, where the objective function is a nested composition of $T$ functions. We assume access to noisy evaluations of the functions and their gradients, through a stochastic first-order oracle. For solving this class of problems, we propose two algorithms using moving-average stochastic estimates, and analyze their convergence to an $\epsilon$-stationary point of the problem. We show that the first algorithm, which is a generalization of [$22$] to the $T$ level case, can achieve a sample complexity of $\mathcal{O}(1/\epsilon^6)$ by using mini-batches of samples in each iteration. By modifying this algorithm using linearized stochastic estimates of the function values, we improve the sample complexity to $\mathcal{O}(1/\epsilon^4)$. This modification also removes the requirement of having a mini-batch of samples in each iteration. To the best of our knowledge, this is the first time that such an online algorithm designed for the (un)constrained multi-level setting, obtains the same sample complexity of the smooth single-level setting, under mild assumptions on the stochastic first-order oracle. </li> </ul>
  
  
  
  
  <li>Anthony Nguyen. <i>Zeroth Order Optimization with zeroth order constraints </i> (in preparation). </li> <ul> <li> This project attempts to investigate a constrained optimization problem where the domain is defined by functions; we examine solutions in the zeroth order setting. </li> </ul>
  
</ol>  






