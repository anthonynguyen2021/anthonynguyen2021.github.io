---
layout: archive
title: "Publications"
permalink: /publications/
author_profile: true
redirect_from:
  - /publications
---


<b> Papers </b> <br> 
<ol>
  <li>Krishnakumar Balasubramanian, Saeed Ghadimi, Anthony Nguyen. <i> Stochastic Multilevel Composition Optimization Algorithms with Level-Independent Convergence Rates </i>, <a href= "https://arxiv.org/pdf/2008.10526.pdf"> arXiv</a>.</li>
  
  <ul> <li> In this paper, we study smooth stochastic multi-level composition optimization problems, where the objective function is a nested composition of $T$ functions. We assume access to noisy evaluations of the functions and their gradients, through a stochastic first-order oracle. For solving this class of problems, we propose two algorithms using moving-average stochastic estimates, and analyze their convergence to an $\epsilon$-stationary point of the problem. We show that the first algorithm, which is a generalization of the paper "A Single Time-Scale Stochastic Approximation Method for Nested Stochastic Optimization" by Ghadimi, Ruszczynski, and Wang to the $T$ level case, can achieve a sample complexity of $\mathcal{O}(1/\epsilon^6)$ by using mini-batches of samples in each iteration. By modifying this algorithm using linearized stochastic estimates of the function values, we improve the sample complexity to $\mathcal{O}(1/\epsilon^4)$. This modification also removes the requirement of having a mini-batch of samples in each iteration. To the best of our knowledge, this is the first time that such an online algorithm designed for the (un)constrained multi-level setting, obtains the same sample complexity of the smooth single-level setting, under mild assumptions on the stochastic first-order oracle. </li> </ul>
  
  
  
  
  <li>Krishnakumar Balasubramanian, Anthony Nguyen. <i>Stochastic Zeroth-order Functional Constrained Optimization </i> (manuscript). </li> <ul> <li> In this paper, we examine a stochastic algorithm that attempts to solve a constrained optimization problem with functional constraints using zeroth order information. To generalize our setting, we seek to solve the dual optimization problem using function and biased derivative estimates. The steps involved in the algorithm utilizes projected gradient descent in each of the iterations. </li> </ul>
  
</ol>  






